---
output: html_document
---
# Study on Predicting Quality of Weightlifting Exercise  
  
  
  
  
    
## SYNOPSIS        
    
  
This is a report on the study of predicting on "how well" a set of 6 participants, in the age group 20-28 years and with limited weightlifting experience, perform the weightlifting exercise using light dumbbells (1.25Kg). There is one "correct" way of doing the exercise (A) and there are 4 (B-E) observable common mistakes as shown below:-  
  
    
A - exactly according to specs  
B - throwing the elbows to the front  
C - lifting the dumbbell only halfway    
D - lowering the dumbbell only halfway  
E - throwing the hips to the front  
  
    
This study is based on the data available from the following HAR study:-  
**Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.**  
  
    
Goal of the study is to fit a prediction model which can, based on the data collected from accelerometers (placed on belt, forearm, arm and dumbbell of the 6 participants), help predict *how well they do* the weightlifting exercise.  
  
    
We first collected the data which had around 160 variables. From this raw data we zoomed into 53 covariates and sliced the training data into training and validation data. Then we built a model based on Random Forest algorithm using K-fold cross validation and arrived at the best model and used it to predict the outcome of the test data for the 20 test cases.  

    
```{r loadLibs, warning=FALSE}
library(caret)
```  

  
  
## DATA COLLECTION   


Let us load the data first.  
  
  
```{r loadData, echo=TRUE,eval=FALSE}
url_train <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url_test <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(url_train, "pml-training.csv")
download.file(url_test, "pml-testing.csv")  
```  
  
```{r readData, echo=TRUE}
trainingData <- read.csv("pml-training.csv",na.strings=c("NA","","#DIV/0!"),header=TRUE)
testingData <- read.csv("pml-testing.csv",na.strings=c("NA","","#DIV/0!"),header=TRUE)

```  
  
As there are many variables in these 2 sets - let us assure the variables are in proper order. The last variable in training data is *classe* (our output of interest) while in testing data it is the *problem_id* (1 to 20). Check that the other variables are in order:-  
  
```{r checkColSeq, echo=TRUE}
nCols <- ncol(trainingData) - 1
trainingCols <- colnames(trainingData)[1:nCols]
testingCols <- colnames(testingData)[1:nCols]
all.equal(trainingCols,testingCols)
```  
  
    
## COVARIATE CREATION    
  
    
Further study of the data shows that large number of NA values exist. Also, there are large number of variables having NA values in all observations. Let us remove those variables first - surely their contribution to model building is nil.  
  
  
```{r coVars1, echo=TRUE}
NAcols <- apply(trainingData,2,function(x) {sum(is.na(x))}) 
trainingData <- trainingData[,which(NAcols == 0)]
NAcols <- apply(testingData,2,function(x) {sum(is.na(x))}) 
testingData <- testingData[,which(NAcols == 0)]
```  
  
    
Now the variables count is *`r ncol(trainingData)`* only.  
Of these, the first set of 7 variables are more like admin variables - not contributing to model building in any way. Let us remove them too.  
  
    
```{r coVars2, echo=TRUE}
trainingData <- trainingData[,8:length(colnames(trainingData))]
testingData <- testingData[,8:length(colnames(testingData))]
```  
  
    
Now the variables count is *`r ncol(trainingData)`* only.  
  
    
See whether there are any variables which have near zero variance across these observations. If so, we can remove them as they do not contribute to build our prediction model.  
  
    
```{r coVars3, echo=TRUE}
nearZeroVars <- nearZeroVar(trainingData,saveMetrics=TRUE)
nearZeroVars
```  
  
    
From the above list we see that none of the remaining variables has near zero variance. So this above set of *`r ncol(trainingData)`* features form our final list for model building for prediction algorithm  
  
    
## TRAIN AND PREDICT  
  
    
We will decide on the algorithm to be **Random Tree** upfront as it is among the more accurate ones as compared to Classification Tree (rpart). We will tweak a couple of model parameters along with cross validation & basic preprocessing to arrive at a best model  
  
  
The preprocessing will be the same - **c("center","scale")** - for all the models  
For train control parameters the resampling method we will use will be - **"cv" the k-fold cross validation** approach. We'll keep changing the value of k.  
  
    
### setup data for training & validation  
  
  
Let us split the training data, keeping 30% for validation  
  
```{r splitData, echo=TRUE}
set.seed(100)
inTrain = createDataPartition(trainingData$classe, p = .7, list=FALSE)
tData = trainingData[inTrain,]
vData = trainingData[-inTrain,]
```  
  
    
### train model with k = 5  

```{r trainRF1, echo=TRUE, eval=TRUE}
set.seed(110)
modFit5 <- train(classe ~., method="rf", data=tData, 
                preProcess=c("center", "scale"), 
                trControl=trainControl(method='cv', number=5, allowParallel=TRUE )
                )
```  
  
    
Let us display the features of this model  
  
    
```{r displayRes1, echo=TRUE}
modFit5$finalModel
vPred <- predict(modFit5, vData)
confusionMatrix(vPred, vData$classe)
```  
  
    
As displayed above, for this model, the **OOB estimate of error rate is 0.74%** and the **prediction accuracy is 0.9939**  
  
    



    
### train model with k = 6  

```{r trainRF2, echo=TRUE, eval=TRUE}
set.seed(110)
modFit6 <- train(classe ~., method="rf", data=tData, 
                preProcess=c("center", "scale"), 
                trControl=trainControl(method='cv', number=6, allowParallel=TRUE )
                )
```  
  
      
      
```{r displayRes2, echo=TRUE}
modFit6$finalModel
vPred <- predict(modFit6, vData)
confusionMatrix(vPred, vData$classe)
```  
  
    
As displayed above, for this model, the **OOB estimate of error rate is 0.71%** and the **prediction accuracy is 0.9941**  
  
    
  
    
### train model with k = 7  

```{r trainRF3, echo=TRUE, eval=TRUE}
set.seed(110)
modFit7 <- train(classe ~., method="rf", data=tData, 
                preProcess=c("center", "scale"), 
                trControl=trainControl(method='cv', number=7, allowParallel=TRUE )
                )
```  
  
    
  
      
      
```{r displayRes3, echo=TRUE}
modFit7$finalModel
vPred <- predict(modFit7, vData)
confusionMatrix(vPred, vData$classe)
```  
  
    
As displayed above, for this model, the **OOB estimate of error rate is 0.79%** and the **prediction accuracy is 0.9935**  
  
    
### Best model  

From the characteristics displayed we notice that the **model with k = 6** has the **minimum value for OOB estimate of error rate = 0.71%** and a **maximum value for overall accuracy = 0.9941**. Therefore we choose this as our best model.  
  
Further characteristics of our chosen model are shown below:-  
  
    
```{r plotBest1, echo=TRUE}
varImpPlot(modFit6$finalModel)
```  
  
   
The above figure depicts the top few features of importance in our best model  
    
```{r plotBest2, echo=TRUE}
plot(modFit6$finalModel,log="y")
legend("top", colnames(modFit6$finalModel$err.rate),col=1:6,cex=0.8,fill=1:6)
```  
  
    
The above figure plots the error in estimating for the best model.  
  
    
  
    
## RESULTS  
  
    
Using our best model, *modFit6*, which is based on K = 6 folds, the prediction on the 20 testcases is as shown below:-  
  
    
```{r predBest, echo=TRUE}
testingDataPred <- predict(modFit6,testingData)
testingDataPred
```  

  
    
    
   

    
  
    
  
    

    
    
